{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNEIVholLnjAzJOJcw99pKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devyadav11/DL_assignment_2/blob/main/DLAssignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "id": "58_eJy67iTqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0039f6-48a9-469e-b063-a10d3f95d95b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZxC9cDHLg8kR"
      },
      "outputs": [],
      "source": [
        "# !pip install opencv-python\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle # for shuffling\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "# !wandb login\n",
        "wandb.login(key=\"500727e3b54d5202c75cd3136d6326f29e89e05f\")\n",
        "\n",
        "#key = 500727e3b54d5202c75cd3136d6326f29e89e05f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaeaLTbjiSkf",
        "outputId": "7a9e1cc6-c4fe-45b8-f71b-dfa529e989b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mee23m074\u001b[0m (\u001b[33mdevyadav11\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEWzL9rAn3od",
        "outputId": "be87e4bc-c29b-4d1e-d67d-8102c3126e44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-19 13:59:20--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.196.207, 74.125.134.207, 74.125.139.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.196.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3816687935 (3.6G) [application/zip]\n",
            "Saving to: ‘nature_12K.zip’\n",
            "\n",
            "nature_12K.zip      100%[===================>]   3.55G   177MB/s    in 26s     \n",
            "\n",
            "2025-04-19 13:59:47 (138 MB/s) - ‘nature_12K.zip’ saved [3816687935/3816687935]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !unzip -q nature_12K.zip"
      ],
      "metadata": {
        "id": "zwjjmnMmQavt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !rm nature_12K.zip"
      ],
      "metadata": {
        "id": "WGPZKCbEQe99"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZAYH2De_Qlad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_path = \"/content/inaturalist_12K\""
      ],
      "metadata": {
        "id": "qKNXSulPUFn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_hight = 256\n",
        "image_width = 256"
      ],
      "metadata": {
        "id": "hFHq27USXd56"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(batchSize):\n",
        "  transform_data = transforms.Compose([ transforms.Resize((image_hight, image_width)), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ])   # normalise = output = (input - mean) / std ) (convert to (-1 to 1)\n",
        "\n",
        "  train_data = datasets.ImageFolder(root = \"/content/inaturalist_12K/train\", transform=transform_data)\n",
        "\n",
        "  class_index = train_data.class_to_idx\n",
        "\n",
        "  training_idx = []\n",
        "  val_idx  = []\n",
        "\n",
        "  for train_class, train_index in class_index.items():\n",
        "    index = []\n",
        "    for k, (img_path, lable) in enumerate(train_data.samples):\n",
        "      if lable == train_index:\n",
        "        index.append(k)\n",
        "\n",
        "\n",
        "    train_indices, val_indices = train_test_split(index, test_size=0.2, random_state=42)\n",
        "    training_idx.extend(train_indices)           # extend creates a single flat list\n",
        "    val_idx.extend(val_indices)\n",
        "\n",
        "  training_data = Subset(train_data, training_idx)\n",
        "  val_data = Subset(train_data, val_idx)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train_loader = DataLoader(training_data, batch_size=batchSize, shuffle=True, pin_memory= True )\n",
        "  val_loader = DataLoader(val_data, batch_size=batchSize, shuffle=True, pin_memory= True)\n",
        "\n",
        "  test_data = datasets.ImageFolder(root = \"/content/inaturalist_12K/val\", transform=transform_data)\n",
        "  test_laoder = DataLoader(test_data, batch_size=batchSize, shuffle=True, pin_memory= True)\n",
        "\n",
        "\n",
        "\n",
        "  return train_loader, val_loader, test_laoder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #return  class_index\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rq_0jRlJjYfG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_accuracy(model, criterion, dataLoader, dataName):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataLoader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f'{dataName} Loss: {val_loss/len(dataLoader)},'\n",
        "          f'{dataName} Accuracy: {100*correct/total}%\\n')\n",
        "\n",
        "    wandb.log({f'{dataName}_accuracy': 100*correct/total})\n",
        "    wandb.log({f'{dataName}_loss': val_loss/len(dataLoader)})"
      ],
      "metadata": {
        "id": "OxhUdG2miwCx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FN3CvdV3sMkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNN(nn.Module):\n",
        "  def __init__(self, num_filters, size_filter, activation_fxn, num_neurons_dense, batch_norm, dropout_prob ):\n",
        "    super(ConvNN, self).__init__()\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    for i in range(len(num_filters)):\n",
        "      if i == 0:\n",
        "        layers.append(nn.Conv2d(in_channels=3, out_channels=num_filters[i], kernel_size=size_filter[i], stride=1, padding=0))\n",
        "        layers.append(activation_fxn)\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        if batch_norm == 'true':\n",
        "          layers.append(nn.BatchNorm2d(num_features=num_filters[i]))\n",
        "      else:\n",
        "        layers.append(nn.Conv2d(in_channels=num_filters[i-1], out_channels=num_filters[i], kernel_size=size_filter[i], stride = 1, padding = 0))\n",
        "        layers.append(activation_fxn)\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        if batch_norm == 'true':\n",
        "          layers.append(nn.BatchNorm2d(num_features=num_filters[i]))\n",
        "\n",
        "      self.conv_stack = nn.Sequential(*layers)\n",
        "      flattened_tensor = torch.flatten(self.conv_stack(torch.zeros(1, 3, image_hight, image_width)))\n",
        "      dim_lastConv = flattened_tensor.shape[-1]\n",
        "      self.flattened = nn.Flatten()\n",
        "      self.dense_layer = nn.Linear(in_features=dim_lastConv, out_features=num_neurons_dense)\n",
        "      self.dropout = nn.Dropout(dropout_prob)\n",
        "      self.output_layer = nn.Linear(in_features=num_neurons_dense, out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_stack(x)\n",
        "    x = self.flattened(x)\n",
        "    x = self.dense_layer(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.output_layer(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def modal_training(optimiser_fn, num_epochs, batch_size, num_filters, size_filter,\n",
        "                   activation_fxn, num_neurons_dense, batch_norm, dropout_prob, weight_decay, learning_rate):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train_loader, val_loader, test_loader = load_data(batch_size)\n",
        "\n",
        "  if activation_fxn == \"relu\":\n",
        "    activation_fxn = nn.ReLU()\n",
        "  elif activation_fxn == \"leaky_relu\":\n",
        "    activation_fxn = nn.LeakyReLU()\n",
        "  elif activation_fxn == \"elu\":\n",
        "    activation_fxn = nn.ELU()\n",
        "  else:\n",
        "    activation_fxn = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  model = ConvNN(num_filters, size_filter, activation_fxn, num_neurons_dense, batch_norm, dropout_prob)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  if optimiser_fn == \"adam\":\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  elif optimiser_fn == \"nadam\":\n",
        "    optimizer = optim.NAdam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  elif optimiser_fn == \"rmsprop\":\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  else:\n",
        " # stocastic gradient decent\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model = torch.nn.DataParallel(model,device_ids = [0]).to(device)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for index, (inputs, labels) in enumerate(tqdm(train_loader, desc=f'Training Progress {epoch+1}')):\n",
        "      inputs, labels = inputs.to(device), labels.to(device)                        # keeps input and lables run on same GPU, or CPU if u choose multiple devices\n",
        "      optimizer.zero_grad()                                                        # set gradients to zero\n",
        "      outputs = model(inputs)                                                      # forward pass\n",
        "      loss = criterion(outputs, labels)                                            # calculate loss\n",
        "      loss.backward()                                                              # backward pass\n",
        "      optimizer.step()                                                             # updates the parameters after back prop\n",
        "\n",
        "\n",
        "    find_accuracy(model, criterion, train_loader, \"train\")\n",
        "    find_accuracy(model, criterion, val_loader, \"validation\")\n",
        "\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ocq5I2j3ZxgK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNDlJ7yp6bam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"assignment_2\")\n",
        "modal = modal_training(optimiser_fn = \"adam\" , num_epochs = 10, batch_size = 64, num_filters = [32,32,32,32,32], size_filter = [3,5,3,5,3],\n",
        "                   activation_fxn = \"elu\", num_neurons_dense = 512, batch_norm = False, dropout_prob = 0.2, weight_decay = 0.005, learning_rate = 1e-4)\n"
      ],
      "metadata": {
        "id": "AmtR-U7O002S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wxJUcCq6B2Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    wandb.init(project=\"assignment_2\")\n",
        "    config = wandb.config\n",
        "    run_name = f\"{config.optimiser}_{config.activation}_{config.num_filters}_{config.batch_size}\"\n",
        "\n",
        "    # Set the run name\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "\n",
        "    # Define and train the model as before\n",
        "    modal_training(learning_rate = config.learning_rate, num_filters = config.num_filters,\n",
        "                size_filter = config.filter_sizes, activation_fxn = config.activation,\n",
        "                optimiser_fn = config.optimiser, num_neurons_dense = config.dense_layer,\n",
        "                weight_decay = config.weight_decay, dropout_prob = config.dropout, batch_norm = False,\n",
        "                batch_size = config.batch_size, num_epochs = 10)\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'sweep cross entropy',\n",
        "    'metric': {\n",
        "      'name': 'validation_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'num_filters': {\n",
        "          'values': [[32,32,32,32,32],[32,64,64,128,128],[128,128,64,64,32],[32,64,128,256,512],[128,128,128,128,128],[32,32,64,64,32,32,128,32]]\n",
        "        },\n",
        "        'filter_sizes': {\n",
        "          'values': [[3,3,3,3,3], [5,5,5,5,5], [3,5,3,5,3]]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values':[0, 0.0005, 0.5]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values':[1e-3,1e-4]\n",
        "        },\n",
        "        'weight_decay': {\n",
        "            'values': [0, 0.0005, 0.005]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0, 0.2, 0.4]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu', 'elu', 'selu']\n",
        "        },\n",
        "        'optimiser': {\n",
        "            'values': ['nadam', 'adam', 'rmsprop']\n",
        "        },\n",
        "        'batch_norm':{\n",
        "            'values': ['true','false']\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32, 64]\n",
        "        },\n",
        "        'dense_layer':{\n",
        "            'values': [128, 256, 512]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config,project='assignment_2')\n",
        "wandb.agent(sweep_id , function = main , count = 3)\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tS-KdAuD7fn-",
        "outputId": "99b7ac93-87f3-4bc8-e466-96148dd58aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: xhodo9zm\n",
            "Sweep URL: https://wandb.ai/devyadav11/assignment_2/sweeps/xhodo9zm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: irp40vu3 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: true\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_layer: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [3, 5, 3, 5, 3]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: [32, 64, 64, 128, 128]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.005\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'assignment_2' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_140112-irp40vu3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/devyadav11/assignment_2/runs/irp40vu3' target=\"_blank\">deep-sweep-1</a></strong> to <a href='https://wandb.ai/devyadav11/assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/devyadav11/assignment_2/sweeps/xhodo9zm' target=\"_blank\">https://wandb.ai/devyadav11/assignment_2/sweeps/xhodo9zm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/devyadav11/assignment_2' target=\"_blank\">https://wandb.ai/devyadav11/assignment_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/devyadav11/assignment_2/sweeps/xhodo9zm' target=\"_blank\">https://wandb.ai/devyadav11/assignment_2/sweeps/xhodo9zm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/devyadav11/assignment_2/runs/irp40vu3' target=\"_blank\">https://wandb.ai/devyadav11/assignment_2/runs/irp40vu3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress 1: 100%|██████████| 125/125 [02:05<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.204473735809326,train Accuracy: 17.6772096512064%\n",
            "\n",
            "validation Loss: 2.203465513885021,validation Accuracy: 18.35%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress 2: 100%|██████████| 125/125 [01:43<00:00,  1.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.1078430280685425,train Accuracy: 23.44043005375672%\n",
            "\n",
            "validation Loss: 2.110389344394207,validation Accuracy: 23.5%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress 3: 100%|██████████| 125/125 [01:45<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.079088381767273,train Accuracy: 24.415551943992998%\n",
            "\n",
            "validation Loss: 2.0837671123445034,validation Accuracy: 25.1%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress 4: 100%|██████████| 125/125 [01:46<00:00,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 2.040353066444397,train Accuracy: 26.80335041880235%\n",
            "\n",
            "validation Loss: 2.0505350157618523,validation Accuracy: 25.45%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress 5: 100%|██████████| 125/125 [01:46<00:00,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 1.982800043106079,train Accuracy: 28.378547318414803%\n",
            "\n",
            "validation Loss: 2.0015586763620377,validation Accuracy: 26.6%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress 6: 100%|██████████| 125/125 [01:46<00:00,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 1.9457154331207276,train Accuracy: 31.55394424303038%\n",
            "\n",
            "validation Loss: 1.9781045950949192,validation Accuracy: 29.05%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress 7: 100%|██████████| 125/125 [01:46<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 1.9240319042205811,train Accuracy: 31.528941117639704%\n",
            "\n",
            "validation Loss: 1.9540326483547688,validation Accuracy: 29.05%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress 8:   2%|▏         | 2/125 [00:01<01:33,  1.32it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my_list = []\n",
        "# f1 = my_list.append(nn.Softmax(dim = 1))\n",
        "# f2 = my_list.append(nn.ReLU())\n",
        "# f3 = my_list.append(nn.LeakyReLU())\n",
        "# f4 = my_list.append(nn.Softmax(dim = 1))\n",
        "# x = [1, 2, 3]\n",
        "# #my_list.extend([4, 5, 6])\n",
        "# #my_list.append([7, 8, 9])\n",
        "\n",
        "# m = nn.Sequential(*my_list)\n",
        "# print(my_list)\n",
        "# z = m(x)"
      ],
      "metadata": {
        "id": "Vx710WoMJ-Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader, val_loader, test_loader = load_data(1000)\n",
        "# for index, (inputs, labels) in enumerate(train_loader):\n",
        "#   print(index, inputs, labels)"
      ],
      "metadata": {
        "id": "xRu0ofJ3f7g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = transforms.Compose([ transforms.Resize((256, 256)), transforms.ToTensor() ])\n"
      ],
      "metadata": {
        "id": "QEtx2aZGXnFH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}